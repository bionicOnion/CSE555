<!DOCTYPE HTML>

<head>
  <meta charset="UTF-8">
  <title>CSE 555: Final Project</title>
  <link rel="stylesheet" href="../cse555.css">
</head>
<body>
  <h1>CSE 555: Final Project</h1>
  <h2>Nonrealistic Rendering with Triangulation</h2>

  <h2>Robert Miller</h2>

  <div class="heading">
    <a href="http://www.cse.wustl.edu/~pless/classes/555/projectFinal.html">Project Guidelines</a>
    ||
    <a href="http://www.cse.wustl.edu/~pless/classes/555/">Course Website</a>
    ||
    <a href="http://bioniconion.github.io/CSE555/ImageWeb/proposal.html">Project Proposal</a>
    <br>
    <br>
    <img src="images/blautopf.jpeg" alt="CSE 555: Final Project" />
    <br>
  </div>

  <hr>


  <!-- PROJECT OVERVIEW -->
  <h3>Project Overview</h3>
  <p>
    The original inspiration for this project was <a href="images/projectIdea.jpeg">this image</a>:
    I intended to generate wireframe-esque "image webs" which conveyed the impression of the
    low-fequency detail of an image exclusively through the use of solid color lines on solid color
    backgrounds. Although this functionality is still at the heart of the application which I
    ultimately developed, experimenting with the finished pipeline and alternative modes of shading
    the triangles produced by the image processing pipeline broadened the scope of what the
    application is capable of. Most notably, the pipeline is capable of automated and almost
    real-time generation of images similar to those produced using <a
    href="https://studiomoniker.com/projects/delaunay-raster">Delaunay Raster</a> (though the
    shading is slightly different and hand-placed points will likely be more aesthetically pleasing
    than randomly sampled ones); an example of this can be seen in the image at the top of the page.
    Because of this shift, I have rebranded this project from "Image Web Filter" to the more
    descriptive&mdash;and accurate to the final project&mdash;title given above.
    <br><br>
    Aside from this main goal of nonrealistic rendering, one of the primary focuses of my work was
    on learning to leverage the GPU to perform hardware-accelerated image processing and rendering.
    A full description of the technologies used can be found in the Pipeline Description and
    Implementation sections below.
    <br><br>
    Further details are available in the project proposal page <a
    href="http://bioniconion.github.io/CSE555/ImageWeb/proposal.html">here</a>.
    <br><br>
    All of the code written for this project can be found in my repository <a
    href="https://github.com/bionicOnion/CSE555/tree/master/ImageWeb">here</a>.
  </p>

  <hr>


  <!-- PIPELINE -->
  <h3>Pipeline Description</h3>
  <p>
    As described in the <a href="http://bioniconion.github.io/CSE555/ImageWeb/proposal.html">
    proposal</a> for this project, generation of the image web is broken down into a
    multi-stage pipeline, the bulk of which is computed on the GPU using Nvidia's CUDA technology.
    The final pipeline (described in more detail in the Implementation section below) is as folows,
    with the middle steps (4 through 7) being repeated for every frame:
    <ol>
      <li>
        <b>Parsing arguments</b>&mdash;Multiple parameters are exposed to the user through the use
        of command-line arguments, allowing granular control over the output produced by the
        application. Although only an image or video are required as inputs (specified with either
        '-i/--image' or '-v/--video'), users can also set foreground and background colors for the
        generated images, which coloring mode to use (from a set of options), the name of the output
        file, the relative weighting applied to various factors of the computation, a seed to use
        for random number generation (used in sampling points for tesselation), and whether or not
        debugging or timing information should be recorded and displayed.
      </li>
      <li>
        <b>Loading image resources</b>&mdash;Images and videos are loaded using OpenCV into an
        <code>ImageResource</code> object, a wrapper which allows both types of input to be handled
        more or less equivalently: any conditional logic relating to saving, loading, or accessing
        the different media types is encapsulated in this one class, simplifying the amount of work
        that code in the main pipeline needs to do. In effect, images are treated as if they were
        single-frame videos in the main pipeline.
      </li>
      <li>
        <b>Pre-allocation of system resources</b>&mdash;The GPU pipeline requires a relatively
        significant amount of memory in a variety of buffers to operate; these buffers are all
        allocated in advance of the main pipeline to allow them to be reused multiple times for
        video processing
      </li>
      <li>
        <b>Generating a distribution</b>&mdash;Using image intensity, detected edges, historical
        positions of points (in the context of video processing), and user-defined weights for the
        relative importance of each of these terms, a two-dimensional distribution is generated to
        control the likelihood of given pixels in the image being sampled as points for use in
        the generated image.
      </li>
      <li>
        <b>Sampling points</b>&mdash;Based on a user-defined parameter for the number of points to
        be sampled relative to the resolution of the image, points are drawn from the distribution
        created in the previous step. Sampling is done by generating two random values in the range
        [0, 1), one of which is used to select the row in the image in which the point will be
        placed, the other of which determines the point location within that row.
      </li>
      <li>
        <b>Point tesselation</b>&mdash;Points are tesselated using a simplified version of the
        algorithm discussed in <a href="http://www.comp.nus.edu.sg/~tants/delaunay2DDownload.html">
        this</a> paper. In particulat, all of the code for transformation to and from discretized
        space has been excluded (as the sampled points are already discrete values), and the final
        steps to convert the solved tesselation to a geometrically correct Delaunay Triangulation
        have been omitted as well: solving for a geometrically perfect minimal triangulation is
        unnecessary in this context, so these steps have been skipped to reduce the amount of
        processing overhead.
        </a>
      </li>
      <li>
        <b>Drawing the generated image</b>&mdash;The triangulation from the previous step stores
        data about the computed triangles into a buffer which is shared between CUDA (as a region
        of memory containing <code>Triangle</code> structs) and OpenGL (as a vertex buffer object).
        This means that the completed triangulation can be drawn immediately, without any overhead
        from moving data around: OpenGL already has a pointer directly to the computed triangles.
        Drawing is done to an offscreen framebuffer; the contents of this framebuffer are then
        copied back to the CPU to be saved and displayed (this is one of the elements of the
        pipeline that could be improved; see the Future Work section below).
      </li>
      <li>
        <b>Releasing of system resources</b>&mdash;At the end of the pipeline, all of the buffers
        allocated on both the host and device are cleanly released after copying the relevant
        results off of the GPU.
      </li>
      <li>
        <b>Displaying/saving results</b>&mdash;Once the pipeline has finished its processing, the
        finished image or video is shown to the user and saved to disk (using either a user-defined
        or automatically generated filename).
      </li>
    </ol>
  </p>

  <hr>


  <!-- IMPLEMENTATION -->
  <h3>Implementation</h3>
  <p>
    All code for the project can be found in my repository <a
    href="https://github.com/bionicOnion/CSE555/tree/master/ImageWeb">here</a>.

    Minimum specs:
    OpenCV 3.0
    Device with CUDA 2.0 capabilities
    OpenGL 4.5
  </p>

  <hr>


  <!-- RESULTS -->
  <h3>Results</h3>
  <p>
    Due to the highly parameterized nature of the system and the relatively subjective and
    randomized quality of the output, the results provided in this section only represent the
    capabilities of this application in the broadest sense. However, these results are still
    useful as a guide to what the system is capable of, including both its basic functionality and
    the set of options exposed to users (explored in more detail in the Parameterization section
    below).

    <h4>Images</h4>

    <b>TODO</b>
    Use the image of Pless, an image of Brookings, Blautopf, the image from formal, high-contrast
    black and white images (from Photo I and II?), low-contrast images, etc.
    Use a variety of different methodologies for creating the images (varied coloring modes, point
    densities, edge to intensity weightings, etc.)
    <br><br>
    One of the things that caught me by surprise was the potential for the images produced by the
    system to fall suddenly and dramatically into the uncanny valley. Images of faces rapidly become
    disconcerting when rendered with a high point density using the color sampling mode: the image
    has a recognizably human shape and coloration, but is distinctly off (in a way that came close
    to body horror for some of the more extreme examples that I came across). I haven't included any
    images of this phenomenon in this report, but I found that a point ratio of somewhere around
    0.03 (meaning that around 3% of pixels will be sampled as points for triangulation) was the
    lower threshold at which images moved from abstract to uncanny.

    <h4>Videos</h4>

    <b>TODO</b>
  </p>

  <hr>


  <!-- BENCHMARKING -->
  <h3>Benchmarking</h3>
  <p>
    Due to the theoretically 'fast' GPGPU implementation of the pipeline described above, I was
    interested to see exactly how fast the application was in practice. As such, I included some
    timing checkpoints (enabled with the flag '-t/--timing') to precisely measure elapsed time
    in the GPU pipeline (as well as an overall timer for the end-to-end performance of the system).
    All of the numbers in this section represent a near-worst case scenario for this project: in
    addition to the code in question being only moderately optimized, the system used for testing is
    built out of aging components, and the perfomance reported here would likely pale in comparison
    to the same code running on newer and higher-end cards such as the GTX 980 Ti or the recently
    announced GTX 1080.
    <br><br>
    That said, the numbers below are still useful as an indication of the relative performance of
    each element of the system and may be useful for drawing conclusions about the viability of
    real-time video processing or targets for optimization work (in addition to providing a basic
    frame of reference for the timing performance of the system).
    <br><br>
    The full specifications of the benchmarking system are as follows:
    <ul>
      <li>CPU: Intel Core i7 3770K @ 3.5 GHz</li>
      <li>GPU: Geforce GTX 670 @ 941 MHz with 2 GB VRAM</li>
      <li>RAM: 16 GB DDR3 @ 2133 MHz</li>
    </ul>
    <b>TODO</b>
  </p>

  <hr>


  <!-- PARAMETERIZATION -->
  <h3>Effects of Parameters</h3>
  <p>
    Vary edge to intensity weighting
    Vary point ratio
    Vary historicity ratio
    Demonstrate a variety of coloring modes
  </p>

  <hr>


  <!-- CHALLENGES -->
  <h3>Challenges Faced</h3>
  <p>
    This is now the second project that I have implemented with GPGPU technology (the first being
    the Prokudin-Gorskii image alignment and recoloring project with which the semester began,
    described <a href="http://bioniconion.github.io/CSE555/ProkudinGorskii/">here</a>); as such, I
    came into this project with some hard-won development experience from my first outing. However,
    GPGPU development is radically different from traditional CPU-based development (which is where
    the bulk of my experience lies), and my prior experience can in no way be considered to apporach
    expertise. As such, the bulk of the issues that I faced in development stemmed from the inherent
    challenges of working in an unfamiliar environment, especially one as foreign and difficult to
    debug as the GPU.
    <br><br>
    The first of these overarching problems was the difficulty of adapting to massively-parallel
    algorithm design. This was one of the main reasons that I sought out an existing solution for
    computing a prefix-sum when calculating the distribution; developing my own implementation which
    was robust enough to stand up to a variety of input types would have eaten up a significant
    amount of development and debugging time which would be only tangentially related to the stated
    goals of the project.
    <br><br>
    A second major issue which arose on multiple occasions was the difficulty of maintaining a
    consistent view of memory throughout the execution of a CUDA kernel. Although these kernels
    logically execute in parallel and some synchronization primitives have been provided (such as
    the life-savingly useful <code>__syncthreads()</code> function), synchronization can only be
    easily achieved between threads within a given block&mdash;and with potentially hundreds of
    blocks running at the mercy of the GPU's scheduler, guaranteeing the ordering of memory accesses
    is extremely difficult, and I found myself running up against bugs on multiple occasions. This is
    in no small part why my implementation has such a large memory footprint on the GPU: the easiest
    way to ensure that memory is consistent is to seperate input and output buffers. Although it
    might be possible to reduce this footprint through more clever reuse of working buffers or more
    advanced synchronization tools, I ultimately didn't think that it was worth focusing on those
    elements of the pipeline (as it's far more important to have a working system than to have a
    beautifully efficient&mdash;but incomplete&mdash;system).
    <br><br>
    One of the other issues which caught me by surprise was the amount of overhead associated with
    the <code>curand_init()</code> function if not used carefully. When I first attempted to use
    cuRAND, I caused my computer to crash multiple times due to the GPU becoming unresponsive while
    it attempted to set up the random number generator (with one of the crashes being severe enough
    that I was afraid that I had done lasting damage to my GPU). Much like developing in
    kernelspace, GPGPU development operates in a high-risk environment without all of the usual
    safety and liveness features guaranteed in a standard userspace application. Although the OS
    and the GPU driver do their best to ensure memory safety and other such features, it's still
    ultimately easier to write dangerously broken code on the GPU.
    <br><br>
    Working with both OpenGL and CUDA simultaneously meant juggling two interdependent and difficult
    systems, neither of which I have used extensively. As such, determining the exact source of bugs
    was frequently difficult (for instance, I was unsure for a long time if the reason that OpenGL
    wasn't drawing anything was because my graphics code was incorrect or because I wasn't feeing
    it any data to draw with).
    <br><br>
    Even without the difficulties of CUDA interoperability, coming to understand OpenGL's default
    behaviors took hours in and of itself (the 'helpful' insertion of word-aligned padding into
    every row of an image when it's being copied was a particular bug that I spent the greater part
    of an evening tracking down).
  </p>

  <hr>


  <!-- FUTURE WORK -->
  <h3>Future Work</h3>
  <p>
    Although this project is ostensibly complete (inasmuch as it is a project for a course which is
    now ending), a significant amount of further development could be&mdash;and potentially will
    be&mdash;done to improve the application. The following list are some of many future avenues for
    future improvement which might make for interesting projects:

    <h4>Using Image Pyramids in Edge Detection</h4>
    As was reported above, edge data was far more useful than intensity data for determining the
    positions at which points should be placed in the image. However, since only the highest
    frequency edge data is computed in the current implementation of the pipeline, relying too
    heavily on edge data will force points too strictly towards these edges. To mitigate this effect
    and to increase the amount of data being used to determine the positions of sampled points,
    edges could be detected at multiple scales through the use of an image pyramid (or another,
    similar technique). Having multi-scale edge data would preserve the nice qualities of points
    sampled on existing edges (since these tend to be the most recognizable featrues of images),
    but with the added feature of drawing from a wider range of possible values (as low-frequency
    edge data appears as a sort of 'smear' when viewed at the original resolution of the image).

    <h4>Improved Voronoi Diagram Generator</h4>
    For the sake of simplicity, my current implementation uses a na&iuml;ve algorithm for computing
    the Voronoi diagram that only marignally leverages the power of massive parallelism. For
    instance, <a
    href="https://pdfs.semanticscholar.org/48f0/fd485615be32f0f37f182b9f2ae4aa18fafc.pdf">this</a>
    paper is the one referenced in the GPU-CDT paper discussed above which they used for efficient
    Delaunay triangulation. Swapping to this (or an equivalent) implementation would alleviate one
    of the most significant bottlenecks in the system as it currently stands. 

    <h4>Optimized Memory Access Patterns</h4>
    Optimizing GPGPU code lies in the subtle art of controlling memory access patterns, an area in
    which I have little to no experience and an aspect of development that I made no particular
    effort to focus on during this project (as a functional application will always outperform a
    fast one). Controlling the sequence of memory accesses and experimenting with different memory
    types (such as __constant__ or __shared__, each of which has the improve system performance by
    50% or more).

    <h4>Expanded Coloring &amp; Triangulation Modes</h4>
    As it stands, only a handful of coloring modes and a single triangulation mode have been
    implemented as part of the pipeline. Attempting to do work with constrained triangulation could
    be beneficial in more accurately copying the source image, improving the quality of the results,
    and extending the options in coloring modes would allow for a wider range of expression in the
    images produced by the application (as well as some potentially valuable experience working with
    more advanced features of OpenGL).

    <h4>Improved Image &amp; Video Code</h4>
    Currently, I am relying on OpenCV for saving, loading, and displaying images; OpenCV's libraries
    for doing each of the above are buggy and incomplete at best and actively missing or
    non-functional at worst. On top of this, relying on OpenCV for this functionality adds an
    otherwise unnecessary dependency to the system without much clear benefit. Transitioning to
    different (or homemade) libraries for saving and loading images and relying on OpenGL for
    displaying them would both improve and simplify the codebase&mdash;a win in every respect.

    <h4>Porting from CUDA to OpenCL</h4>
    Much like my desire to move away from OpenCV discussed above, moving from CUDA to OpenCL would
    remove one of the major limitations on the usability of the application. On top of that, porting
    the code would give me an opportunity to become familiar with OpenCL, broadening my
    understanding and ability with respect to GPGPU development.
  </p>


</body>
